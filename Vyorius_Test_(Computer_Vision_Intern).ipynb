{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLbpGdT9IvAT"
      },
      "source": [
        "token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vMtFPBpIxM9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from huggingface_hub import login\n",
        "login(token=\"your--token\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-izh7po5Lj0R"
      },
      "source": [
        "for fast processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOV04ROvNB4l"
      },
      "source": [
        "above is successful for static video, now\n",
        "\n",
        "for run time detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As2qZSEkY5ii"
      },
      "source": [
        "the correct verion of this above code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNdfopy_Y-gi"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"frame_size\": (640, 480),  # Reduced processing resolution\n",
        "    \"frame_skip\": 3,  # Process every 3rd frame\n",
        "    \"confidence_threshold\": 0.7,\n",
        "    \"half_precision\": True,\n",
        "    \"batch_size\": 4  # For future batch processing\n",
        "}\n",
        "\n",
        "# Initialize pipeline with optimizations\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "torch_dtype = torch.float16 if (device == 0 and CONFIG[\"half_precision\"]) else torch.float32\n",
        "\n",
        "detector = pipeline(\n",
        "    model=\"google/owlvit-base-patch32-v2\",\n",
        "    task=\"zero-shot-object-detection\",\n",
        "    device=device,\n",
        "    torch_dtype=torch_dtype\n",
        ")\n",
        "\n",
        "# Optimized categories\n",
        "custom_categories = [\n",
        "    \"lightbulb\",  # Simplified descriptions\n",
        "    \"matchstick\",\n",
        "    \"monitor\",\n",
        "    \"lion\",\n",
        "    \"gaming console\"\n",
        "]\n",
        "\n",
        "def smart_resize(frame):\n",
        "    \"\"\"Maintain aspect ratio while resizing\"\"\"\n",
        "    h, w = frame.shape[:2]\n",
        "    scale = CONFIG[\"frame_size\"][0] / w\n",
        "    return cv2.resize(frame, (int(w*scale), int(h*scale)))\n",
        "\n",
        "def process_frame(frame, prev_predictions=None):\n",
        "    \"\"\"\n",
        "    Optimized frame processing with prediction caching\n",
        "    \"\"\"\n",
        "    # Resize first to reduce processing load\n",
        "    resized_frame = smart_resize(frame)\n",
        "    orig_h, orig_w = frame.shape[:2]\n",
        "    resized_h, resized_w = resized_frame.shape[:2]\n",
        "\n",
        "    # Convert to PIL once\n",
        "    pil_image = Image.fromarray(cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Only process if no valid cached predictions\n",
        "    if prev_predictions is None:\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            predictions = detector(\n",
        "                pil_image,\n",
        "                text_prompts=custom_categories,\n",
        "                threshold=CONFIG[\"confidence_threshold\"]\n",
        "            )\n",
        "    else:\n",
        "        predictions = prev_predictions\n",
        "\n",
        "    # Scale boxes back to original dimensions\n",
        "    scale_x = orig_w / resized_w\n",
        "    scale_y = orig_h / resized_h\n",
        "\n",
        "    annotated_frame = frame.copy()\n",
        "    for pred in predictions['boxes']:\n",
        "      if pred['score'] < CONFIG[\"confidence_threshold\"]:\n",
        "        continue\n",
        "        box = pred\n",
        "        # Scale coordinates\n",
        "        xmin, ymin, xmax, ymax = map(int, box)\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(annotated_frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
        "\n",
        "        # Label with confidence\n",
        "        label = f\"{pred['label']} {pred['score']:.2f}\"\n",
        "        cv2.putText(annotated_frame, label, (xmin, ymin-10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "\n",
        "    return annotated_frame, predictions\n",
        "\n",
        "def main(video_source=\"/content/drive/MyDrive/bulb_sample_video.mp4\"):\n",
        "    cap = cv2.VideoCapture(video_source)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video\")\n",
        "        return\n",
        "\n",
        "    # Get original video properties\n",
        "    orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Calculate actual output FPS\n",
        "    output_fps = fps / CONFIG[\"frame_skip\"]\n",
        "\n",
        "    # Video writer setup\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(\"optimized_output.mp4\", fourcc, output_fps,\n",
        "                         (orig_width, orig_height))\n",
        "\n",
        "    frame_count = 0\n",
        "    prev_predictions = None\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        if frame_count % CONFIG[\"frame_skip\"] != 0:\n",
        "            # Skip frame but reuse previous predictions\n",
        "            out.write(frame)\n",
        "            continue\n",
        "\n",
        "        # Process frame\n",
        "        processed_frame, prev_predictions = process_frame(frame, prev_predictions)\n",
        "        out.write(processed_frame)\n",
        "\n",
        "        # Optional display\n",
        "        cv2_imshow(processed_frame)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    # from moviepy.editor import VideoFileClip\n",
        "    # clip = VideoFileClip(\"optimized_video.mp4\")\n",
        "    # clip.preview()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osb4Q37ANKw3",
        "outputId": "4ab23b9c-ac3f-4e4a-8e06-c758e4352ed6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Unable to access the webcam.\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "\n",
        "# Initialize the zero-shot object detection pipeline\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "detector = pipeline(model=\"google/owlvit-base-patch32\", task=\"zero-shot-object-detection\", device=device)\n",
        "\n",
        "# Define custom object categories\n",
        "custom_categories = [\n",
        "    \"a photo of a lightbulb\",\n",
        "    \"a photo of a matchstick\",\n",
        "    \"a photo of a monitor\",\n",
        "    \"a photo of a lion\",\n",
        "    \"a photo of a gaming console\"\n",
        "]\n",
        "\n",
        "def process_frame(frame):\n",
        "    \"\"\"\n",
        "    Process a single video frame for zero-shot object detection.\n",
        "    \"\"\"\n",
        "    # Convert OpenCV frame to PIL Image\n",
        "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Perform zero-shot object detection\n",
        "    predictions = detector(pil_image, candidate_labels=custom_categories)\n",
        "\n",
        "    # Annotate the frame with bounding boxes and labels\n",
        "    for pred in predictions:\n",
        "        box = pred['box']\n",
        "        label = pred['label']\n",
        "        score = pred['score']\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(frame,\n",
        "                      (int(box['xmin']), int(box['ymin'])),\n",
        "                      (int(box['xmax']), int(box['ymax'])),\n",
        "                      (0, 255, 0), 2)\n",
        "\n",
        "        # Add label and confidence score\n",
        "        text = f\"{label}: {score:.2f}\"\n",
        "        cv2.putText(frame, text,\n",
        "                    (int(box['xmin']), int(box['ymin']) - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "\n",
        "    return frame\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to capture video from webcam and perform real-time object detection.\n",
        "    \"\"\"\n",
        "    # Open webcam (use 0 for default camera)\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Unable to access the webcam.\")\n",
        "        return\n",
        "\n",
        "    print(\"Press 'q' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(\"Error: Unable to read from webcam.\")\n",
        "            break\n",
        "\n",
        "        # Process the current frame for object detection\n",
        "        annotated_frame = process_frame(frame)\n",
        "\n",
        "        # Display the annotated frame in real-time\n",
        "        cv2.imshow(\"Real-Time Object Detection\", annotated_frame)\n",
        "\n",
        "        # Exit on pressing 'q'\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isT9KXpwdrpR"
      },
      "source": [
        "another different one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd2Jzih5duTa"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load a pre-trained model (e.g., TensorFlow Lite or custom model)\n",
        "model_path = \"path_to_your_model.tflite\"  # Replace with your model path\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Load the reference image for one-shot detection (non-COCO object)\n",
        "reference_image_path = \"path_to_reference_image.jpg\"  # Replace with your reference image path\n",
        "reference_image = cv2.imread(reference_image_path)\n",
        "reference_image = cv2.resize(reference_image, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n",
        "reference_image = np.expand_dims(reference_image, axis=0).astype(np.float32)\n",
        "\n",
        "# Initialize webcam for live video feed\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to grab frame\")\n",
        "        break\n",
        "\n",
        "    # Preprocess frame for model input\n",
        "    input_frame = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n",
        "    input_frame = np.expand_dims(input_frame, axis=0).astype(np.float32)\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_frame)\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Extract predictions\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # Display predictions on the live video feed\n",
        "    for detection in output_data:\n",
        "        confidence = detection[2]\n",
        "        if confidence > 0.5:  # Threshold for confidence level\n",
        "            x_min, y_min, x_max, y_max = detection[3:7]\n",
        "            x_min, y_min = int(x_min * frame.shape[1]), int(y_min * frame.shape[0])\n",
        "            x_max, y_max = int(x_max * frame.shape[1]), int(y_max * frame.shape[0])\n",
        "\n",
        "            # Draw bounding box and label on the frame\n",
        "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "            label = f\"Object: {confidence:.2f}\"\n",
        "            cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "            # Print predictions to console\n",
        "            print(f\"Detected object at [{x_min}, {y_min}, {x_max}, {y_max}] with confidence {confidence:.2f}\")\n",
        "\n",
        "    # Display the live video feed with detections\n",
        "    cv2.imshow(\"Live Object Detection\", frame)\n",
        "\n",
        "    # Break loop on 'q' key press\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izcQhQOifBOG"
      },
      "source": [
        "3.rd verion of code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}